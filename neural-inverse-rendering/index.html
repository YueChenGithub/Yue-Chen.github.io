
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Neural Inverse Rendering</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <meta property="og:image" content="https://jonbarron.info/zipnerf/img/nottingham.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/zipnerf/"/>
    <meta property="og:title" content="Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields" />
    <meta property="og:description" content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields" />
    <meta name="twitter:description" content="Neural Radiance Field training can be accelerated through the use of grid-based representations in NeRF's learned mapping from spatial coordinates to colors and volumetric density. However, these grid-based approaches lack an explicit understanding of scale and therefore often introduce aliasing, usually in the form of jaggies or missing scene content. Anti-aliasing has previously been addressed by mip-NeRF 360, which reasons about sub-volumes along a cone rather than points along a ray, but this approach is not natively compatible with current grid-based techniques. We show how ideas from rendering and signal processing can be used to construct a technique that combines mip-NeRF 360 and grid-based models such as Instant NGP to yield error rates that are 8%-77% lower than either prior technique, and that trains 24x faster than mip-NeRF 360." />
    <meta name="twitter:image" content="https://jonbarron.info/zipnerf/img/teaser.jpg" /> -->


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’¡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Neural Scene Decomposition for Accurate Light and Material Reconstruction via Physically-Based Global Illumination Estimation</br> 
                <small>
                Master Thesis, 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://yuechengithub.github.io/">
                          Yue Chen
                        </a>
                    </li>
                    <li>
                        <a href="https://peter-kocsis.github.io/">
                            Peter Kocsis
                        </a>
                    </li>
                    <li>
                        <a href="https://niessnerlab.org/">
                            Matthias NieÃŸner
                        </a>
                    </li>
                   
                    </br>Technical University of Munich
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://github.com/YueChenGithub/neural-inverse-rendering/blob/main/docs/MA_Yue_Chen.pdf">
                            <image src="img/thesis_icom.png" height="60px">
                                <h4><strong>Thesis</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="https://github.com/YueChenGithub/neural-inverse-rendering">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>

                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <image src="img/overview.jpg" class="img-responsive" alt="overview" width="100%" >
            </div>
        </div>




        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                     Recent advances in neural rendering have achieved pinpoint reconstruction of 3D scenes from multi-view images. To enable scene editing under different lighting conditions, an increasing number of methods are integrating differentiable surface rendering into the pipelines. However, many of these methods rely heavily on simplified surface rendering algorithms, while considering primarily direct lighting or fixed indirect illumination only. We introduce a more realistic rendering pipeline that embraces multi-bounce Monte-Carlo path tracing. Benefiting from the multi-bounce light path estimation, our method can decompose high-quality material properties without necessitating additional prior knowledge. Additionally, our model can accurately estimate and reconstruct secondary shading effects, such as indirect illumination and self-reflection. We demonstrate the advantages of our model to baseline methods qualitatively and quantitatively across synthetic and real-world scenes.
                </p>
            </div>
        </div>

        <br>
<!-- 
        <div class="row ">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Material Model
                </h3>

                <img src="img/material_model.png" class="img-responsive center-block" alt="overview" style="max-width: 80%; display: block; margin-left: auto; margin-right: auto;">

                <p class="text-justify">
                    AAA
                </p>

            </div>
        </div>

        <br> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Multi-bounce Monte-Carlo Ray Tracing
                </h3>


                <img src="img/ray_tracing.png" class="img-responsive center-block" alt="overview" style="max-width: 50%; display: block; margin-left: auto; margin-right: auto;">





                <table style="width: 100%; border-collapse: collapse;">
                    <tr>
                        <td style="text-align: center;">
                            <video class="video" width=100% id="bounce" loop playsinline autoplay muted src="img/bounce.mp4" onplay="resizeAndPlay(this)"></video>
                            <canvas height=0 class="videoMerge" id="bounceMerge"></canvas>
                        </td>
                        <td style="text-align: center;">
                            <image src="img/gt.jpg" class="img-responsive" alt="overview" width="100%" >
                        </td>
                    </tr>

                    
                <p class="text-justify">
                    We develop a differentiable inverse path tracer that enables the simulation of light bouncing multiple times within a scene, utilizing Monte Carlo sampling. The simulation of multi-bounce light transport enables a precise reconstruction of the material properties.  By simulating an increased number of bounces, the precision of the reconstructed albedo is notably improved. This results in a reduction of indirect lighting being inaccurately embedded into the albedo.
                </p>
                

                <table class="bounce-table">
                    <tr>
                        <td>Estimated Albedo</td>
                        <td>GT Albedo</td>
                    </tr>
                </table>


                

            </div>
        </div>


        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Relighting Results Synthetic Data
                </h3>

                <video id="v0" width="100%" autoplay loop muted>
                    <source src="img/lego_relighting.mp4" type="video/mp4" />
                </video>

                <table class="relighting-table">
                    <tr>
                        <td>Baseline</td>
                        <td>Ours</td>
                        <td>Ground Truth</td>
                    </tr>
                </table>

                <video id="v1" width="100%" autoplay loop muted>
                    <source src="img/hotdog_relighting.mp4" type="video/mp4" />
                </video>

                <table class="relighting-table">
                    <tr>
                        <td>Baseline</td>
                        <td>Ours</td>
                        <td>Ground Truth</td>
                    </tr>
                </table>

                <p class="text-justify">
                    In comparison to the baseline approach, our method features more accurate color representation, enhanced realism in specularity, and significantly improved secondary lighting effects (e.g. self-reflection).
                </p>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Relighting Results Real-world Data
                </h3>

                <image src="img/dtu.png" class="img-responsive" alt="overview" width="100%" ></image>

                <table class="dtu-table">
                    <tr>
                        <td>Reference</td>
                        <td>Relighting 1</td>
                        <td>Relighting 2</td>
                        <td>Relighting 3</td>
                    </tr>
                </table>


            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bounce
                </h3>
                <table style="width: 100%; border-collapse: collapse;">
                    <tr>
                        <td style="text-align: center;">
                            <video class="video" width=100% id="lego" loop playsinline autoplay muted src="img/lego_exp.mp4" onplay="resizeAndPlay(this)"></video>
                            <canvas height=0 class="videoMerge" id="legoMerge"></canvas>
                        </td>
                        <td style="text-align: center;">
                            <video id="v0" width="100%" autoplay loop muted>
                                <source src="img/lego_gt.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                    <tr>
                        <td style="text-align: center;">
                            <video class="video" width=100% id="hotdog" loop playsinline autoplay muted src="img/hotdog_exp.mp4" onplay="resizeAndPlay(this)"></video>
                            <canvas height=0 class="videoMerge" id="hotdogMerge"></canvas>
                        </td>
                        <td style="text-align: center;">
                            <video id="v0" width="100%" autoplay loop muted>
                                <source src="img/hotdog_gt.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>

                <table class="bounce-table">
                    <tr>
                        <td>Estimated Albedo</td>
                        <td>GT Albedo</td>
                    </tr>
                </table>

                
                <p class="text-justify">
                    AAA
                </p>
            </div>
        </div> -->
        
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Material Editing Results
                </h3>

                <video id="v0" width="100%" autoplay loop muted>
                    <source src="img/material_editing.mp4" type="video/mp4" />
                </video>

                <p class="text-justify">
                    We can also control the diffuse color and the amount of speucularity.
                </p>
            </div>
        </div>

        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                Thanks to Peter Kocsis for his guidance, thanks to Minghua Chen for her supports, and to Mohamed Ebbed for his advice.  Special thanks to my cats, Huajiao and Danta, for their emotional support.
                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
